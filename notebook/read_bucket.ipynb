{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azure-storage-blob\n",
    "#!pip install xml-python\n",
    "#!pip install bs4\n",
    "#!pip install BeautifulSoup\n",
    "#!pip install lxml\n",
    "#!pip install pandas\n",
    "#pip install fastparquet\n",
    "#!pip install BytesIO\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!c:/Users/marco/AppData/Local/Programs/Python/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Connect to the storage account\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=<your_account_name>;AccountKey=<your_account_key>;EndpointSuffix=core.windows.net\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get a reference to the raw container and file\n",
    "raw_container_name = \"<your_raw_container_name>\"\n",
    "raw_blob_name = \"<your_raw_blob_name>\"\n",
    "raw_blob_client = blob_service_client.get_blob_client(container=raw_container_name, blob=raw_blob_name)\n",
    "\n",
    "# Download the file contents as a string\n",
    "raw_file_contents = raw_blob_client.download_blob().content_as_text()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading XML Data Danfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import pytz\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import io\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÕES GLOBAIS\n",
    "def read_context(json_path: str):\n",
    "    \"\"\"\n",
    "    Read json content in order to get context variables from data process.\n",
    "    Args:\n",
    "        json_path: path from where json file containing project\n",
    "            information is.\n",
    "    \"\"\"\n",
    "\n",
    "    context_file = open(json_path)\n",
    "    context = json.load(context_file)\n",
    "\n",
    "    return context\n",
    "\n",
    "def set_dtypes(df, dtypes):\n",
    "    for column in dtypes:\n",
    "        #print(column)\n",
    "        df[column] = df[column].astype(dtypes[column])\n",
    "    return df\n",
    "\n",
    "def convert_float_to_int(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].astype(np.int64)\n",
    "    return df\n",
    "\n",
    "def data_load():\n",
    "    brasil_fuso_horario = pytz.timezone(\"America/Sao_Paulo\")\n",
    "    data_atual = datetime.now(brasil_fuso_horario).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return data_atual\n",
    "\n",
    "def cria_data_fonte_e_carga(df,coluna):\n",
    "    df['data_fonte'] = pd.to_datetime(df[coluna], format='%Y-%m-%d %H:%M:%S',errors='ignore').dt.strftime('%Y-%m-%d %H:%M')\n",
    "    df['data_carga']  = data_load()\n",
    "    return df\n",
    "\n",
    "def cria_coluna_year_month_day(df,coluna: str):\n",
    "    df[\"year\"] = df[coluna].str.slice(0,4)\n",
    "    df[\"month\"] = df[coluna].str.slice(5,7)\n",
    "    df[\"day\"] = df[coluna].str.slice(8,10)\n",
    "    return df\n",
    "\n",
    "def connect_storage_account():\n",
    "    context = read_context('../context/transform-info.json')\n",
    "    connect_str = context['transform-info'][0]['connect_string']\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "    return blob_service_client\n",
    "\n",
    "def get_container_xml_raw(container : str, blob : str):\n",
    "    blob_service_client = connect_storage_account()\n",
    "    raw_container_client = blob_service_client.get_container_client(container)\n",
    "\n",
    "    # Especificando o caminho do arquivo CSV no seu bucket\n",
    "    blob_client = raw_container_client.get_blob_client(blob)\n",
    "\n",
    "    # Lendo o arquivo CSV em um objeto pandas DataFrame\n",
    "    #csv_data = blob_client.download_blob().content_as_text()\n",
    "    xml = blob_client.download_blob()\n",
    "    return xml\n",
    "\n",
    "def create_data_frame_danfe(xml):\n",
    "    # define o namespace\n",
    "    namespace = {'nfe': 'http://www.portalfiscal.inf.br/nfe'}\n",
    "\n",
    "    # carrega o arquivo XML\n",
    "    tree = ET.parse(xml)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # extrai os dados das tags\n",
    "    cUF = root.find('.//nfe:cUF', namespace).text\n",
    "    cNF = root.find('.//nfe:cNF', namespace).text\n",
    "    numero_nf = root.find('.//nfe:nNF', namespace).text\n",
    "    data_emissao = root.find('.//nfe:dhEmi', namespace).text\n",
    "    natOp = root.find('.//nfe:natOp', namespace).text\n",
    "    CNPJ_emit = root.find('.//nfe:CNPJ', namespace).text\n",
    "    nome_emitente = root.find('.//nfe:emit/nfe:xNome', namespace).text\n",
    "    logadouro_emitente = root.find('.//nfe:emit/nfe:enderEmit/nfe:xLgr', namespace).text\n",
    "    numero_emitente = root.find('.//nfe:emit/nfe:enderEmit/nfe:nro', namespace).text\n",
    "    bairro_emitente  = root.find('.//nfe:emit/nfe:enderEmit/nfe:xBairro', namespace).text\n",
    "    fone_emitente  = root.find('.//nfe:emit/nfe:enderEmit/nfe:fone', namespace).text\n",
    "    nome_cliente = root.find('.//nfe:dest/nfe:xNome', namespace).text\n",
    "\n",
    "    # cria o DataFrame com as colunas desejadas\n",
    "    df = pd.DataFrame(columns=['codigo_produto', 'descricao_produto', 'ncm', 'valor', 'quantidade','base_calculo_icms','aliquota_icms','valor_icms'])\n",
    "\n",
    "    # itera sobre todas as tags <det> para extrair os dados de cada produto\n",
    "    for det in root.findall('.//nfe:det', namespace):\n",
    "        prod_dict = {\n",
    "            'codigo_produto': det.find('nfe:prod/nfe:cProd', namespace).text,\n",
    "            'descricao_produto': det.find('nfe:prod/nfe:xProd', namespace).text,\n",
    "            'ncm': det.find('nfe:prod/nfe:NCM', namespace).text,\n",
    "            'valor': det.find('nfe:prod/nfe:vProd', namespace).text,\n",
    "            'quantidade': det.find('nfe:prod/nfe:qCom', namespace).text,\n",
    "            'base_calculo_icms' : det.find('nfe:imposto/nfe:ICMS/nfe:ICMS00/nfe:vBC', namespace).text,\n",
    "            'aliquota_icms' : det.find('nfe:imposto/nfe:ICMS/nfe:ICMS00/nfe:pICMS', namespace).text,\n",
    "            'valor_icms' : det.find('nfe:imposto/nfe:ICMS/nfe:ICMS00/nfe:vICMS', namespace).text\n",
    "\n",
    "        }\n",
    "        \n",
    "        # adiciona o dicionário ao DataFrame\n",
    "        df = df.append(prod_dict, ignore_index=True)\n",
    "    df['codigo_uf_emitente'] = cUF\n",
    "    df['codigo_nf'] = cNF\n",
    "    df['numero_nf'] = numero_nf\n",
    "    df['data_emissao'] = data_emissao\n",
    "    df['natureza_operacao'] = natOp\n",
    "    df['cnpj_emitente'] = CNPJ_emit\n",
    "    df['nome_emitente'] = nome_emitente\n",
    "    df['logadouro_emitente'] = logadouro_emitente\n",
    "    df['numero_emitente'] = numero_emitente       \n",
    "    df['bairro_emitente'] = bairro_emitente       \n",
    "    df['fone_emitente'] = fone_emitente         \n",
    "    df['nome_cliente'] = nome_cliente\n",
    "    return df\n",
    "    \n",
    "def save_container_processing_parquet(df,blob_service_client):\n",
    "    # SAve parquet processing .get_blob_client('danfe.parquet')\n",
    "    processing_container_name = blob_service_client.get_container_client(\"processing\")\n",
    "    parquet_blob_name = 'danfe.parquet'\n",
    "    parquet_blob_name_bytes = bytes(parquet_blob_name, 'utf-8')\n",
    "    parquet_blob_client = blob_service_client.get_container_client(processing_container_name).get_blob_client(parquet_blob_name_bytes)\n",
    "\n",
    "    with io.BytesIO() as output:\n",
    "        df.to_parquet(output, partition_cols=['year', 'month', 'day'], index=False, engine='fastparquet')\n",
    "        data = output.getvalue()\n",
    "    \n",
    "    # Envia os dados para o blob storage\n",
    "    with io.BytesIO(data) as input:\n",
    "        parquet_dataset = pq.ParquetDataset(input)\n",
    "        parquet_blob_client.upload_blob(parquet_dataset.read().to_pybytes(), overwrite=True)\n",
    "\n",
    "    # Envia os dados para o blob storage\n",
    "    #parquet_blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "\n",
    "    #parquet_blob_data = df.to_parquet(parquet_blob_client, partition_cols=['year', 'month', 'day'], index=False, engine='fastparquet')\n",
    "    #parquet_blob_client.upload_blob(parquet_blob_data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_convert_int = ['quantidade','aliquota_icms']\n",
    "data_types_nfe = {\n",
    "      'codigo_produto' : np.int64,\n",
    "      'descricao_produto' : object,                 \n",
    "      'ncm' : object,\n",
    "      'valor' : np.float64,\n",
    "      'quantidade' : np.float64,\n",
    "      'base_calculo_icms': np.float64,\n",
    "      'aliquota_icms' : np.float64,\n",
    "      'valor_icms' : np.float64,\n",
    "      'codigo_uf_emitente' : np.int64,\n",
    "      'codigo_nf' : np.int64,\n",
    "      'numero_nf' : np.int64,\n",
    "      'data_emissao' : np.datetime64,\n",
    "      'natureza_operacao' : object ,\n",
    "      'cnpj_emitente' : object,\n",
    "      'nome_emitente' : object,\n",
    "      'logadouro_emitente' : object,\n",
    "      'numero_emitente' : object,\n",
    "      'bairro_emitente' : object,\n",
    "      'fone_emitente' : object,\n",
    "      'nome_cliente' : object      \n",
    "   }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes Leitura XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('cobase-nfe2.xml', parser = ET.XMLParser(encoding = 'utf-8'))\n",
    "root = ET.parse('cobase-nfe2.xml',).getroot()\n",
    "\n",
    "nsNFE = {'ns': \"http://www.portalfiscal.inf.br/nfe\"}\n",
    "\n",
    "numero_nfe = root.find('ns:NFe/ns:infNFe/ns:ide/ns:cNF',nsNFE).text\n",
    "natOp = root.find('ns:NFe/ns:infNFe/ns:ide/ns:natOp',nsNFE).text\n",
    "chave_nfe = root.find('ns:NFe/ns:infNFe',nsNFE).attrib['Id'][3:]\n",
    "\n",
    "# Dados Emissor\n",
    "emit_cnpj = root.find('ns:NFe/ns:infNFe/ns:emit/ns:CNPJ',nsNFE).text\n",
    "emit_nome = root.find('ns:NFe/ns:infNFe/ns:emit/ns:xNome',nsNFE).text\n",
    "emit_lgr = root.find('ns:NFe/ns:infNFe/ns:emit/ns:enderEmit/ns:xLgr',nsNFE).text\n",
    "emit_nro = root.find('ns:NFe/ns:infNFe/ns:emit/ns:enderEmit/ns:nro',nsNFE).text\n",
    "emit_xBairro = root.find('ns:NFe/ns:infNFe/ns:emit/ns:enderEmit/ns:xBairro',nsNFE).text\n",
    "\n",
    "cProd = root.findall('ns:NFe/ns:infNFe/ns:det/ns:prod/ns:cProd',nsNFE)\n",
    "\n",
    "\n",
    "#print(numero_nfe.text, chave_nfe)\n",
    "\n",
    "dados = {\n",
    "    \"numero_nfe\" : [numero_nfe],\n",
    "    \"natOp\" : [natOp],\n",
    "    \"chave_nfe\" : [chave_nfe],\n",
    "    \"emit_cnpj\":[emit_cnpj],\n",
    "    \"emit_nome\":[emit_nome],\n",
    "    \"emit_lgr\":[emit_lgr],\n",
    "    \"emit_nro\":[emit_nro],\n",
    "    \"emit_xBairro\":[emit_xBairro]\n",
    "}\n",
    "#dados\n",
    "lista = []\n",
    "dic = {}\n",
    "for p in root.iter('nItem'):\n",
    "    #dic['cProd'] = p.find('prod/cProd').text\n",
    "    #lista.append(dic)\n",
    "    print(p.text)\n",
    "#print(lista)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = open('cobase-nfe2.xml')\n",
    "nfe = minidom.parse(xml)\n",
    "\n",
    "lista = []\n",
    "produtos = nfe.getElementsByTagName('cProd')\n",
    "xProd = nfe.getElementsByTagName('xProd')\n",
    "vProd = nfe.getElementsByTagName('vProd')\n",
    "\n",
    "for p in produtos:\n",
    "    dic = {}\n",
    "    dic['cod_prod'] = p.firstChild.data    \n",
    "    lista.append(dic)\n",
    "    #print(p.firstChild.data)\n",
    "for xProd in xProd:\n",
    "    dic = {}\n",
    "    dic['xProd'] = xProd.firstChild.data    \n",
    "    lista.append(dic)\n",
    "for vProd in vProd:\n",
    "    dic = {}\n",
    "    dic['vProd'] = vProd.firstChild.data    \n",
    "    lista.append(dic)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produtos = nfe.getElementsByTagName('cProd')\n",
    "xProd = nfe.getElementsByTagName('xProd')\n",
    "lista = [{'cod_prod': p.firstChild.data} for p in produtos] + [{'xProd': x.firstChild.data} for x in xProd]\n",
    "lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cobase-nfe2.xml') as xml:\n",
    "    nfe = minidom.parse(xml)\n",
    "\n",
    "produtos = nfe.getElementsByTagName('cProd') + \\\n",
    "           nfe.getElementsByTagName('xProd')\n",
    "lista = [{'cod_prod': p.firstChild.data} if p.tagName == 'cProd' else {'xProd': p.firstChild.data} for p in produtos]\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('cobase-nfe2.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "produtos = root.findall('.//det')\n",
    "\n",
    "lista = [{'cod_prod': p.find('prod/cProd').text,\n",
    "          'xProd': p.find('prod/xProd').text,\n",
    "          'vProd': p.find('prod/vProd').text} for p in produtos]\n",
    "lista\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = open('cobase-nfe2.xml')\n",
    "nfe = minidom.parse(xml)\n",
    "\n",
    "lista = []\n",
    "produtos = nfe.getElementsByTagName('det')\n",
    "\n",
    "for produto in produtos:\n",
    "    cod_prod = produto.getElementsByTagName('cProd')[0].firstChild.data\n",
    "    xProd = produto.getElementsByTagName('xProd')[0].firstChild.data\n",
    "    vProd = produto.getElementsByTagName('vProd')[0].firstChild.data\n",
    "    dic = {'cod_prod': cod_prod, 'xProd': xProd, 'vProd': vProd}\n",
    "    lista.append(dic)\n",
    "\n",
    "xml.close()\n",
    "lista\n",
    "\n",
    "\n",
    "#pd.DataFrame(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "\n",
    "#numero_nfe = root.find()\n",
    "parser = etree.XMLParser(recover=True,encoding='utf-8') #iso-8859-5\n",
    "root = ET.parse('cobase-nfe.xml',parser=parser).getroot()\n",
    "\n",
    "nsNFE = {'ns': \"http://www.portalfiscal.inf.br/nfe\"}\n",
    "numero_nfe = root.find('ns:NFe/ns:infNFe/ns:ide/ns:nNF',nsNFE)\n",
    "chave_nfe = root.find('ns:NFe/ns:infNFe',nsNFE).attrib['Id'][3:]\n",
    "\n",
    "#tree = etree.parse('danfe.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(numero_nfe.text)\n",
    "#print(chave_nfe.attrib['Id'][3:])\n",
    "print(chave_nfe)\n",
    "#numero_nfe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = open('danfe.xml')\n",
    "nfe = minidom.parse(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Carrega o XML\n",
    "tree = ET.parse('cobase-nfe2.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Obtém as informações da nota fiscal\n",
    "cUF = root.find('.//{http://www.portalfiscal.inf.br/nfe}cUF').text\n",
    "cNF = root.find('.//{http://www.portalfiscal.inf.br/nfe}cNF').text\n",
    "natOp = root.find('.//{http://www.portalfiscal.inf.br/nfe}natOp').text\n",
    "CNPJ_emit = root.find('.//{http://www.portalfiscal.inf.br/nfe}emit/{http://www.portalfiscal.inf.br/nfe}CNPJ').text\n",
    "xNome_emit = root.find('.//{http://www.portalfiscal.inf.br/nfe}emit/{http://www.portalfiscal.inf.br/nfe}xNome').text\n",
    "\n",
    "# Cria uma lista com as informações dos produtos\n",
    "produtos = []\n",
    "for det in root.findall('.//{http://www.portalfiscal.inf.br/nfe}det'):\n",
    "    cProd = det.find('{http://www.portalfiscal.inf.br/nfe}prod/{http://www.portalfiscal.inf.br/nfe}cProd').text\n",
    "    xProd = det.find('{http://www.portalfiscal.inf.br/nfe}prod/{http://www.portalfiscal.inf.br/nfe}xProd').text\n",
    "    NCM = det.find('{http://www.portalfiscal.inf.br/nfe}prod/{http://www.portalfiscal.inf.br/nfe}NCM').text\n",
    "    vUnCom = det.find('{http://www.portalfiscal.inf.br/nfe}prod/{http://www.portalfiscal.inf.br/nfe}vUnCom').text\n",
    "    produtos.append({'cProd': cProd, 'xProd': xProd, 'NCM': NCM, 'vUnCom': vUnCom})\n",
    "\n",
    "# Cria o DataFrame pandas\n",
    "df = pd.DataFrame(produtos)\n",
    "df['cUF'] = cUF\n",
    "df['cNF'] = cNF\n",
    "df['natOp'] = natOp\n",
    "df['CNPJ_emit'] = CNPJ_emit\n",
    "df['xNome_emit'] = xNome_emit\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def extract_data_from_xml(xml_string):\n",
    "    root = ET.fromstring(xml_string)\n",
    "\n",
    "    # Extrair informações do cabeçalho\n",
    "    header = root.find('.//{http://www.portalfiscal.inf.br/nfe}ide')\n",
    "    data = {\n",
    "        'cUF': header.find('cUF').text,\n",
    "        'cNF': header.find('cNF').text,\n",
    "        'natOp': header.find('natOp').text,\n",
    "        'CNPJ': header.find('.//{http://www.portalfiscal.inf.br/nfe}CNPJ').text,\n",
    "        'xNome': header.find('.//{http://www.portalfiscal.inf.br/nfe}xNome').text\n",
    "    }\n",
    "\n",
    "    # Extrair informações dos produtos\n",
    "    products = root.iter('{http://www.portalfiscal.inf.br/nfe}det')\n",
    "    for product in products:\n",
    "        data.update({\n",
    "            f\"cProd_{product.get('nItem')}\": product.find('prod/cProd').text,\n",
    "            f\"xProd_{product.get('nItem')}\": product.find('prod/xProd').text,\n",
    "            f\"NCM_{product.get('nItem')}\": product.find('prod/NCM').text,\n",
    "            f\"vUnCom_{product.get('nItem')}\": product.find('prod/vUnCom').text\n",
    "        })\n",
    "\n",
    "    # Criar o DataFrame a partir do dicionário de dados\n",
    "    df = pd.DataFrame.from_dict([data])\n",
    "\n",
    "    return df\n",
    "extract_data_from_xml('cobase-nfe2.xml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código final Transformação Nota fiscal XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main ():\n",
    "    blob_service_client = connect_storage_account()\n",
    "    xml = get_container_xml_raw('raw','cobase-nfe2.xml')\n",
    "    df = create_data_frame_danfe(xml)\n",
    "    df = set_dtypes(df, data_types_nfe)\n",
    "    df = convert_float_to_int(df,columns_convert_int)\n",
    "    df = cria_data_fonte_e_carga(df, 'data_emissao')\n",
    "    df = cria_coluna_year_month_day(df, 'data_fonte')    \n",
    "    save_container_processing_parquet(df,blob_service_client)\n",
    "    return df\n",
    "\n",
    "\n",
    "main()\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['data_fonte'] = pd.to_datetime(df['data_emissao'], format='%Y-%m-%d %H:%M:%S',errors='ignore').dt.strftime('%Y-%m-%d %H:%M')\n",
    "#df['data_carga']  = data_load()\n",
    "df.head()\n",
    "#df.dtypes\n",
    "#df.to_parquet(destination_bucket_name, partition_cols=['YEAR', 'MONTH', 'DAY'],engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_file = open('../context/transform-info.json')\n",
    "context = json.load(context_file)\n",
    "a = context['transform-info'][0]['connect_string']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_str = \"\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "\n",
    "raw_container_client = blob_service_client.get_container_client(\"raw\")\n",
    "# Especificando o caminho do arquivo CSV no seu bucket\n",
    "blob_client = raw_container_client.get_blob_client(\"cobase-nfe2.xml\")\n",
    "\n",
    "# Lendo o arquivo CSV em um objeto pandas DataFrame\n",
    "#csv_data = blob_client.download_blob().content_as_text()\n",
    "xml = blob_client.download_blob()\n",
    "\n",
    "tree = ET.parse(xml)\n",
    "root = tree.getroot()\n",
    "\n",
    "#df = pd.read_csv(csv_data, sep=';')\n",
    "processing_container_name = blob_service_client.get_container_client(\"processing\")\n",
    "parquet_blob_client = blob_service_client.get_container_client(processing_container_name).get_blob_client('file.parquet')\n",
    "df.to_parquet(parquet_blob_client, partition_cols=['year', 'month', 'day'], index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando data lake azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "\n",
    "# Configurando a conexão com o seu bucket na Azure\n",
    "connect_str = \"\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "container_client = blob_service_client.get_container_client(\"landing\")\n",
    "\n",
    "# Especificando o caminho do arquivo CSV no seu bucket\n",
    "blob_client = container_client.get_blob_client(\"teste.csv\")\n",
    "\n",
    "# Lendo o arquivo CSV em um objeto pandas DataFrame\n",
    "#csv_data = blob_client.download_blob().content_as_text()\n",
    "csv_data = blob_client.download_blob()\n",
    "df = pd.read_csv(csv_data, sep=';')\n",
    "\n",
    "# Exibindo o DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movendo arquivo teste.csv da landing para RAW\n",
    "landing_container_client = blob_service_client.get_container_client(\"landing\")\n",
    "landing_blob_client = landing_container_client.get_blob_client(\"teste.csv\")\n",
    "\n",
    "raw_container_client = blob_service_client.get_container_client(\"raw\")\n",
    "raw_blob_client = raw_container_client.get_blob_client(\"teste.csv\")\n",
    "raw_blob_client.start_copy_from_url(landing_blob_client.url)\n",
    "\n",
    "\n",
    "#landing_blob_client.delete_blob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_blob_client.get_blob_properties()\n",
    "status = raw_blob_client.get_blob_properties().copy.status\n",
    "\n",
    "if raw_blob_client.get_blob_properties().copy.status == 'success':\n",
    "    print(\"sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modificacao = data['last_modified']\n",
    "data_modificacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastparquet import *\n",
    "# LENDRO DA RAW E SALVANDO NA PROCESSING EM FORMATO PARQUET\n",
    "\n",
    "raw_container_client = blob_service_client.get_container_client(\"raw\")\n",
    "# Especificando o caminho do arquivo CSV no seu bucket\n",
    "raw_blob_client = raw_container_client.get_blob_client(\"teste.csv\")\n",
    "\n",
    "processing_container_client = blob_service_client.get_container_client(\"processing\")\n",
    "processing_blob_client = processing_container_client.get_blob_client(\"teste.parquet\")\n",
    "\n",
    "# Lendo o arquivo CSV em um objeto pandas DataFrame\n",
    "#csv_data = blob_client.download_blob().content_as_text()\n",
    "csv_data = raw_blob_client.download_blob()\n",
    "df = pd.read_csv(csv_data, sep=';')\n",
    "\n",
    "# Exibindo o DataFrame\n",
    "\n",
    "parquet_blob_data = df.to_parquet('teste.parquet',engine=\"fastparquet\")\n",
    "processing_blob_client.upload_blob(parquet_blob_data, overwrite=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
